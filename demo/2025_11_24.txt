4.阵列 32*32 能跑到1Ghz，WNS +7.8ps，
u_cu_dq_cnt_reg[0]/CLK到u_core/sys_inst/row_pe_gen[10].col_pe_gen[31].pe_inst_pe_psum_out_reg[25]/D
的路径延时986ps
IO 路径的时序非常宽松，slack 600多

5.64*64 目标500Mhz，余量+637 ps，
理论最高频率: 数据路径延迟为 1355 ps 。理论上，该设计在当前综合策略下的最高频率约为 738 MHz (1000/1.355)。



第一周把除了axi的部分都搞完了，然后测试
第二周把axi也写了，放一块综合

FIFO weight做了个register
m k n小于W


1.布线一直跑不出来，跑了好几个晚上，后面我就只进行综合看时序了
2.综合的话，可能是input_buffer的问题，没能正确将其正确识别为RAM，可能是因为写的时候为了方便写成了三读一写的端口。但是由于为了流水线重叠增加吞吐量，对于ABC可能需要同时去读，所以在想要不拆成三个
综合的结果就是把input_buffer和output_buffer黑盒化了，不然input_buffer全用寄存器在实现
然后axi和apb的逻辑还需要改
3.没写过axi和apb，我会有正确结果，但我不确定对不对。但是，如果排除axi_slave axi_master 和 apb的话，tpu是能通过的了测试的


核心架构	Systolic Array
阵列尺寸 16*16 (可参数化，systolic_array_width 或者 W)
数据精度	输入 INT8 / 累加 INT32
总线接口	AXI4 (数据) + APB3 (配置)
综合频率 （基于 ASAP7 工艺库综合评估）32*32 1GHz+；64*64 700MHz+

具体说明：
1.D = A * B + C，记E = A * B
2.支持 M K N < W的所有如上矩阵运算
3.A * B部分由systolic模块进行，E + C部分由systolic下游的vpu模块进行
4.可以实现连续的矩阵加载和运算，只要M K N不发生变化。
比如，我的FIFO中有A1B1C1读取地址和D1的写回地址，A2B2C2&D2， A3B3C3&D3...
只要A1A2A3...每个A矩阵的M和K分别相同（B的K N和C的M N同理），就可以连续计算，并且流水线是重叠的，不需要计算完A1 * B1 + C1后排空systolic再进行A2 * B2 + C2
对于systolic即B1输入-->switch-->A1输入，计算A1 * B1，输入B2-->switch-->A2输入，计算A2 * B2，输入B3
5.实现方式是双权重寄存器 (Active/Inactive)（For B），一个前台计算，一个后台加载，用switch信号切换
6.control_unit可以连续工作直到FIFO中的任务全部完成

目前问题
1.不使用axi的时候，直接把数据放在ram里，让tpu测试结果没问题；使用axi之后有正确的输出结果，但是不知道是否符合整个协议的要求
同时因为不知道外部的带宽，所以我写的时候也是用参数写的，64 128等等都可以直接调整
2.ram 必须要一读一写的话，因为我设计流水线重合的，所以A B C的存储得分开，不然影响吞吐


老师好，这是我目前的进度，基本上晨皓学长的要求都满足了，请您过目，刚才组会上可能讲的不是很清楚
然后就是我想问下，我接下来是去联系佳兴学长继续优化这个嘛，还是问下晨皓学长还有什么要求




